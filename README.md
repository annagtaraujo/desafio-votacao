# Desafio de Infra

Autora: Anna Araujo

Cria√ß√£o do documento: 10/04/2025 

## 1. Ferramentas utilizadas

As ferramentas utilizadas para a cria√ß√£o deste ambiente foram:

- Minikube
- Ngrok
- Terraform
- Github Actions
- Helm

Este ambiente para o desafio foi criado pensando em uma maior otimiza√ß√£o de custos (para n√£o utilizar clouds p√∫blicas).

## 1.1 Arquitetura

![Arquitetura do Laborat√≥rio](image.png)

Na arquitetura mostrada na imagem acima, pode demonstrar que:
- O usu√°rio votante acessar√° o frontend pelo seu respectivo service
- O nginx que est√° dentro da imagem Docker do frontend tem em seu nginx.conf as rotas que precisam ser chamadas no service do backend
- O deployment backend-votacao (acess√≠vel pelo seu respectivo service), valida o desafio anti-rob√¥ (que, neste laborat√≥rio, √© uma opera√ß√£o de soma simples para diminui√ß√£o de complexidade) e publica em uma fila Redis
- O worker, que √© uma outra aplica√ß√£o que comp√µe o ecossistema do backend, consome essa fila do Redis e publica os votos no PostgreSQL


## 2. Configura√ß√µes de Uso

### 2.1 Cria√ß√£o e exposi√ß√£o do Minikube local para uso do Github Actions

1) Considerando que o Minikube roda localmente, o primeiro desafio foi torn√°-lo vis√≠vel para uma esteira de CI no Github Actions. Para tal, foi necess√°rio instalar o Minikube na m√°quina local, utilizando a [documenta√ß√£o oficial](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download).

2) Ap√≥s a instala√ß√£o, √© poss√≠vel inicializar o ambiente com:
```
minikube start
```

3) Com o minikube operante, j√° √© poss√≠vel come√ßar a fazer um proxy para fazer a exposi√ß√£o do cluster local.

4) Antes de expor o cluster para o Github Actions, por√©m, √© necess√°rio aplicar conceitos de:

- Cluster Role Binding
- Service Account
- Secrets

Antes eles estavam sendo criados por manifestos no diret√≥rio k8s/config/, mas foi feita uma modifica√ß√£o para que estes recursos fossem criados via Terraform (exceto a secret, que possui uma limita√ß√£o por um erro no Terraform que n√£o reconhece que o Service Account est√° criado, portanto apenas a secret ser√° criada como manifesto). 

Assim, o Minikube j√° estar√° com as permiss√µes prontas para que o github tenha a devida permiss√£o de acesso ao cluster. √â importante observar que, no cluster role binding, a permiss√£o deve ser do tipo "cluster-admin":

```
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

Para que o user do github possa aplicar as configura√ß√µes via Actions. Acesse o diret√≥rio **k8s/config** para mais detalhes sobre o manifesto da secret e o **infra/dev** para a configura√ß√£o da infra do Minikube como c√≥digo.

Executa-se localmente o manifesto da secret no minikube:

```
kubectl apply -f secret-github-sa.yaml
```

5) A secret criada gerar√° um token, que ser√° parte fundamental da configura√ß√£o. Para retornar esse token, executar localmente:
```
kubectl -n kube-system get secret github-sa-token -o jsonpath="{.data.token}" | base64 -d && echo
```

6) Esse token dever√° estar atualizado no kubeconfig criado localmente para a cria√ß√£o do contexto no cluster para o Github. O arquivo **ngrok.yaml** ficar√° assim:
```
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: <endere√ßo-de-exposi√ß√£o-do-ngrok>
    insecure-skip-tls-verify: true
  name: minikube
contexts:
- context:
    cluster: minikube
    user: terraform-sa
    namespace: default
  name: minikube
current-context: minikube
users:
- name: terraform-sa
  user:
    token: <token-retornado-no-passo-anterior>
```
**Esse arquivo ficar√° localizado na m√°quina local, no .kube**

Para testar o funcionamento da config, pode-se executar o comando:
``` 
KUBECONFIG=ngrok.yaml kubectl get pods -A
```

Assim, √© poss√≠vel comprovar que o novo contexto est√° funcional.

7) Com essas informa√ß√µes em m√£os, j√° √© poss√≠vel buscar as √∫ltimas pe√ßas para a montagem de permiss√µes. J√° posso come√ßar expondo o meu minikube com:
```
kubectl proxy --address=0.0.0.0 --accept-hosts='.*'
```

8) O ngrok ajudar√° a expor o cluster de maneira que ele seja acess√≠vel ao github. Para tanto, √© necess√°rio instalar a CLI do ngrok e fazer uma conta gratuita (que, mesmo com as limita√ß√µes, atender√° a este laborat√≥rio). Instalar de acordo com a [documenta√ß√£o oficial do ngrok](https://dashboard.ngrok.com/get-started/setup/linux)

9) Ap√≥s a instala√ß√£o da CLI do ngrok, pode-se expor o ambiente local utilizando:
```
ngrok http 8001
```

10) Com a exposi√ß√£o completa, surgir√° uma URL do tipo:
```
https://abcd-efgh-ijk-lmno-pqrs-tu-vwxy.ngrok-free.app
```

Essa URL √© a pe√ßa faltante par terminar a configura√ß√£o do ngrok.yaml do passo 6.

11) Agora que o Minikube tem o kubeconfig (que aqui √© chamado ngrok.yaml) que cria o contexto para a exposi√ß√£o via ngrok, j√° √© poss√≠vel torn√°-lo conhecido pelo github. Para isso, pode-se criar uma secret no reposit√≥rio do github clicando em **"settings" > "secrets" > "actions"**, onde ser√° poss√≠vel criar um key=value com o ngrok.yaml codificado em base64. Extrai-se o base64 com:
```
base64 -w 0 ngrok.yaml > kubeconfig_b64.txt
```

E copia-se o conte√∫do do arquivo **kubeconfig_b64.txt** para uma chave de mesmo nome no secrets do reposit√≥rio:

Ou seja: Para o github, a key value, ser√°:
``` 
KUBECONFIG_B64=<conte√∫do-do-kubeconfig_b64.txt>
```

### **Observa√ß√£o importante!** 
Todas as vezes que o minikube for exposto via ngrok, **o ngrok.yaml, bem como o base64 e a secret no github, dever√£o ser atualizados!** 
Isso se deve pela volatilidade do endere√ßo criado pelo ngrok toda vez que ele √© exposto. Esse endere√ßo √© atualizado no ngrok.yaml, que consequentemente gera um base64 diferente, que dever√° ser conhecido pelo Github.

12) Agora que j√° est√° tudo configurado do lado do ambiente local e do github, pode-se come√ßar a pensar no workflow para deployar as aplica√ß√µes de frontend e backend. O arquivo de configura√ß√£o da esteira que atender√° o minikube √© o **.github/workflows/deploy-dev.yaml**

### 2.2 Gerenciamento da infraestrutura

No diret√≥rio **infra/dev** √© possivel encontrar 2 arquivos de configura√ß√£o do terraform, que setam o minikube como provider e criam os namespaces, service account e cluster role bindings via terraform. Por enquanto, s√≥ est√£o sendo criados os namespaces *dev* e *monitoring*.

### 2.3 Cria√ß√£o de imagens Docker e build-push via Github Actions

1) Foram criadas 2 aplica√ß√µes simples de backend e frontend. Elas consistem em:

Frontend: Recebe requests de usu√°rios, que ter√£o que votar entre 2 op√ß√µes dispon√≠veis

Backend: Recebe e computa os votos em um banco de dados simples

Esses arquivos que comp√µem a aplica√ß√£o podem ser encontrados no diret√≥rio **apps/backend e apps/frontend**, bem como seus Dockerfiles, que ser√£o montados pelo workflow **build-push.yaml** e "empurrados" para o Dockerhub correspondente.

### **Observa√ß√£o importante!** 
Para que o Github Actions saiba como logar no Docker Hub, ser√° necess√°ro criar outras 2 secrets que permitir√£o essa intera√ß√£o. S√£o elas:
- DOCKERHUB_TOKEN
- DOCKERHUB_USERNAME

A cria√ß√£o do token para o Dockerhub pode ser feita acessando a conta do Dockerhub e clicando no canto superior direito em **"Settings" > "Personal access tokens" > "New access token"**.

### 2.4 Cria√ß√£o de deployments e services

No diret√≥rio **k8s/dev/deployment**, encontram-se os arquivos yaml que criam o conjunto de deployment e service das aplica√ß√µes, utilizando a imagem que foi "buildada" e "empurrada" pelo workflow do Github Actions. Esses arquivos s√£o aplicados utilizando workflows do Github Actions. Mais detalhes sobre o funcionamento do workflow de deploy podem ser encontrados no item 2.1 deste documento.

A secret necess√°ria para a autentica√ß√£o no PostgreSql contida em **/k8s/config** deve ser criada utilizando:

```
kubectl apply -f secret-postgres.yaml
```

√â importante relembrar a raz√£o: este laborat√≥rio apresentou uma limita√ß√£o na cria√ß√£o de secrets via IaC e, portanto, est√£o sendo criadas **antes** dos deployments, como prepara√ß√£o do ambiente.

### 2.5 Monitoramento

O monitoramento foi criado utilizando a kube-prometheus-stack do Helm. Isso se deve ao fato de que a stack j√° possui todos os par√¢metros necess√°rios para que o Service Discovery funcione.

Seus manifestos podem ser encontrados no path **/k8s/monitoring**. Eles s√£o aplicados via workflow, pelo **deploy-monitoring-stack-v2.yaml**.

O monitoramento l√™ dados expostos pelo:

- Backend (/metrics no PodMonitor do backend-deploy.yaml)
- Redis (k8s/dev/deployment/redis-exporter-deploy.yaml)
- PostgreSql (k8s/dev/deployment/postgres-exporter-deploy.yaml)

Com isso, √© poss√≠vel ver:

#### 2.5.1 M√©tricas da aplica√ß√£o do backend (sistema de vota√ß√£o)

O backend possui algumas m√©tricas instrumentadas do tipo prometheus_client do Python:

‚úÖ Total de votos por op√ß√£o:
promql:
```
votos_total
```

‚úÖ Separando o somat√≥rio por op√µes dispon√≠veis para voto:

promql:
```
sum by (opcao) (votos_total)
```

‚è±Ô∏è Tempo m√©dio de resposta das requisi√ß√µes:
promql:
```
rate(flask_http_request_duration_seconds_sum[1m]) 
/ 
rate(flask_http_request_duration_seconds_count[1m])
```

‚ùå N√∫mero de falhas por c√≥digo HTTP:
promql
```
sum by (status_code) (rate(flask_http_request_total{status_code!~"2.."}[5m]))
```

#### 2.5.2 M√©tricas do Redis (via redis-exporter)

O Redis est√° com m√©tricas expostas (via redis-exporter):

üìä Tamanho da fila de votos:
Pelo nome da fila (votos):

promql:
```
redis_list_length{key="votos"}
```

üîÅ Taxa de enfileiramento:
promql
```
rate(redis_commands_total{command="lpush"}[1m])
```

‚úÖ Taxa de consumo:
promql
```
rate(redis_commands_total{command="rpop"}[1m])
```

#### 2.5.3 M√©tricas do PostgreSQL

Via postgres-exporter:

üìÑ Conex√µes ativas:
promql
```
pg_stat_activity_count
```

üîÅ Transa√ß√µes por segundo:
promql
```
rate(pg_stat_database_xact_commit[1m]) 
+ 
rate(pg_stat_database_xact_rollback[1m])
```

### 3. Pontos de Melhoria (WIP)

Alguns pontos de melhoria para este laborat√≥rio:

- Criar um chart para deployar os servi√ßos do app via Helm com toda a estrutura necess√°ria (Services, ServiceMonitor, PodMonitor, e afins)
- Melhorar a seguran√ßa das secrets (apesar que, para um laborat√≥rio local, funciona razoavelmente bem)
- Automatizar a exposi√ß√£o do Minikube via ngrok e atualiza√ß√£o do ngrok.yaml com um script em bash ou em python

### 4. Refer√™ncias

- [Deploy Kubernetes resources in a Minikube Cluster](https://dev.to/chefgs/deploy-kubernetes-resources-in-minikube-cluster-using-terraform-1p8o)
- [Docker: Use Access Tokens](https://docs.docker.com/security/for-developers/access-tokens/)
- [Ngrok: Setup and Installation](https://dashboard.ngrok.com/get-started/setup/linux)
- [Terraform Registry: Cluster Role Binding](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/cluster_role_binding_v1)
- [Terraform Registry: Service Account](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/service_account)
- [How to create a monitoring stack using kube-prometheus-stack](https://medium.com/israeli-tech-radar/how-to-create-a-monitoring-stack-using-kube-prometheus-stack-part-1-eff8bf7ba9a9)